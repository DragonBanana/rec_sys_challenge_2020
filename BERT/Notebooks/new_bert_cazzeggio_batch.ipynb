{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Produce tweet sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7q3caWkj6XYV"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If there's a GPU available use it, otherwise use the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "x8TixXKb_7_N",
    "outputId": "bf891cb4-c050-43fa-ddf8-3c430118a82e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ROWS = 100\n",
    "BATCH_SIZE = 100\n",
    "# PAD = int(0)\n",
    "MODEL = 'bert-base-multilingual-cased'\n",
    "MODEL_PATH = 'models/' + MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "whpZS8YeVLAj"
   },
   "outputs": [],
   "source": [
    "PATH = \"tweet_tokens/text_tokens_padded_50k.csv\"\n",
    "RESULT_PATH = \"embeddings/sentence_embeddings_50k_batch.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the tweets length after being padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"max_tweet_length.txt\", \"r\") as f:\n",
    "    MAX_LENGTH = int(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "511"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LENGTH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT tokenizer using the 'bert-multilingual-cased' vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 64
    },
    "colab_type": "code",
    "id": "9tyUmC5WFo4l",
    "outputId": "ba3d0d00-e592-4dcd-db0e-6209e8ced3ff"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "adZeKDvO2UQM"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "t43f4yIP2pey",
    "outputId": "78f20b6b-6e60-4bac-b14e-2bd4da58ba76"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119547"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "B73jja022WUp",
    "outputId": "8636a93d-5e1c-46fe-fd77-be2a0ddf71da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab['[PAD]']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Yv9TGRwMAJ8"
   },
   "source": [
    "## Create the model from a pre-trained one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lb7-UgxuMFCQ"
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\n",
    "    MODEL_PATH, # Use the 12-layer BERT model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "colab_type": "code",
    "id": "zrwdAOZxy2Me",
    "outputId": "5401c30f-251f-44c3-d6f7-2e935d11a288"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 199 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "embeddings.word_embeddings.weight                       (119547, 768)\n",
      "embeddings.position_embeddings.weight                     (512, 768)\n",
      "embeddings.token_type_embeddings.weight                     (2, 768)\n",
      "embeddings.LayerNorm.weight                                   (768,)\n",
      "embeddings.LayerNorm.bias                                     (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "encoder.layer.0.attention.self.query.weight               (768, 768)\n",
      "encoder.layer.0.attention.self.query.bias                     (768,)\n",
      "encoder.layer.0.attention.self.key.weight                 (768, 768)\n",
      "encoder.layer.0.attention.self.key.bias                       (768,)\n",
      "encoder.layer.0.attention.self.value.weight               (768, 768)\n",
      "encoder.layer.0.attention.self.value.bias                     (768,)\n",
      "encoder.layer.0.attention.output.dense.weight             (768, 768)\n",
      "encoder.layer.0.attention.output.dense.bias                   (768,)\n",
      "encoder.layer.0.attention.output.LayerNorm.weight             (768,)\n",
      "encoder.layer.0.attention.output.LayerNorm.bias               (768,)\n",
      "encoder.layer.0.intermediate.dense.weight                (3072, 768)\n",
      "encoder.layer.0.intermediate.dense.bias                      (3072,)\n",
      "encoder.layer.0.output.dense.weight                      (768, 3072)\n",
      "encoder.layer.0.output.dense.bias                             (768,)\n",
      "encoder.layer.0.output.LayerNorm.weight                       (768,)\n",
      "encoder.layer.0.output.LayerNorm.bias                         (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "pooler.dense.weight                                       (768, 768)\n",
      "pooler.dense.bias                                             (768,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-2:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tell pytorch to run this model on the GPU.\n",
    "#model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YwBEqBu8Eyk2"
   },
   "source": [
    "### Function used to save the embeddings to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o7UdeUVEc0e2"
   },
   "outputs": [],
   "source": [
    "def save_embeddings(index, embs):\n",
    "\n",
    "    for emb_list in embs:\n",
    "      string = str(index) + ','\n",
    "\n",
    "      for item in emb_list:\n",
    "          string += str(item) + '\\t'\n",
    "          \n",
    "      embeddings_file.write(string + '\\n')\n",
    "\n",
    "      index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda used to convert a list of strings separated by '\\t' into a numpy array of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZkxFFXPjK4Fi"
   },
   "outputs": [],
   "source": [
    "f_to_int = lambda x: int(x)\n",
    "f_int = lambda x: np.array(list(map(f_to_int, x.replace('\\n','').split('\\t'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open the needed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "biYeZ-b3dYmV"
   },
   "outputs": [],
   "source": [
    "embeddings_file = open(RESULT_PATH, \"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "GQgL4uGjdZe5",
    "outputId": "62c8fcba-31a6-43a7-ac49-7457520b048f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_file.write(\"tweet_features_tweet_id,sentence_embeddings\\n\")  # write the header of the embeddings file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g5XSTF6QHwKS"
   },
   "outputs": [],
   "source": [
    "tokens_file = open(PATH, \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hswdaVD04d2K"
   },
   "outputs": [],
   "source": [
    "# model.eval()  # is this needed ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bqzlicjI2VAS"
   },
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tweets in chunks, produce and save their sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xX9R4yLpIFjj",
    "outputId": "3c270ef6-0a77-44b4-98ad-df8bc2af3d8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk : 0\n",
      "CPU times: user 5min 4s, sys: 1min 9s, total: 6min 14s\n",
      "Wall time: 35.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# ~20 MINUTES EXECUTION ON THE 50k TWEETS\n",
    "\n",
    "# ignore header\n",
    "tokens_file.readline()\n",
    "\n",
    "\n",
    "input_tokens = np.zeros(shape=(CHUNKSIZE, MAX_LENGTH), dtype=np.int64)\n",
    "masks = np.zeros(shape=(CHUNKSIZE, MAX_LENGTH), dtype=np.int64)\n",
    "\n",
    "finished = False\n",
    "i = 0\n",
    "\n",
    "while not finished and i < N_ROWS:\n",
    "\n",
    "    print('chunk : ' + str(int(i/BATCH_SIZE)))\n",
    "\n",
    "    # BUILD A BATCH\n",
    "    j = 0\n",
    "    while not finished and j < BATCH_SIZE:\n",
    "        \n",
    "        line = str(tokens_file.readline())\n",
    "      \n",
    "        if i+j >= N_ROWS or line == '':\n",
    "            finished = True\n",
    "        \n",
    "        elif line != '':\n",
    "            line = line.split(',')[1]\n",
    "            input_tokens[j] = f_int(line)\n",
    "\n",
    "            # create attention mask and convert both the mask\n",
    "            #  and the input tokens to pytorch tensors\n",
    "            masks[j] = np.array(input_tokens[j] > 0)\n",
    "      \n",
    "        j += 1\n",
    "    \n",
    "\n",
    "    # BUILD PYTORCH TENSORS FOR THE \n",
    "    # BATCH AND MOVE THEM TO GPU\n",
    "    masks_tensor = torch.tensor(masks)\n",
    "    input_tensor = torch.tensor(input_tokens)\n",
    "\n",
    "    # move tensors to GPU\n",
    "    input_tensor = input_tensor.to(device) \n",
    "    masks_tensor = masks_tensor.to(device) \n",
    "\n",
    "    # PROCESS THE BATCH\n",
    "    outputs = model(input_ids=input_tensor, attention_mask=masks_tensor)\n",
    "\n",
    "    embeddings = outputs[1].tolist()\n",
    "    \n",
    "    save_embeddings(i, embeddings)\n",
    "\n",
    "    #del input_tensor\n",
    "    #del masks_tensor\n",
    "    #del embeddings\n",
    "    #torch.cuda.empty_cache()\n",
    "\n",
    "    i += BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wGtFkDoOJl4Z"
   },
   "outputs": [],
   "source": [
    "tokens_file.close()\n",
    "\n",
    "embeddings_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "new_bert_cazzeggio.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
